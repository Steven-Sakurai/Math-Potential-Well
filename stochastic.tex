\documentclass[a4paper, 11pt]{article}
\usepackage[top=1.0in, bottom=1.0in, left=1.2in, right=1.2in]{geometry} 
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage[]{braket}
\title{\textbf{Stochastic Analysis}}
\author{Steven Shen}
\begin{document}
\maketitle

\section{Foundation}
A stochastic process is a collection of $R.V.$: $X = \{X_t; 0 \leq t < \infty\}$ on sample space $(\Omega, \mathcal{F})$, which take values in a second measurable state space $(\mathcal{R}^d, \mathcal{B}(\mathcal{R}^d))$.

\subsection{Understanding $\sigma-algebra$}

\subsection{Filtration}
A non-decreasing family $\{ \mathcal{F}_t; t \geq 0 \}$ of $sub-\sigma-field$ of $\mathcal{F}$: $\mathcal{F}_s \subseteq \mathcal{F}_t \subseteq \mathcal{F}$ for $0 \leq s < t < \infty$. Set $\mathcal{F}_{\infty} = \sigma(\bigcup_{t \geq 0}\mathcal{F}_t)$.\\
\indent Given a process $X(t)$, the simplest choice of a filtration is $\mathcal{F}_t ^X = \sigma(X_s; 0 \leq s \leq t)$.

\subsection{Conditional Expectation}
$\mathbb{E}[X|\mathcal{G}]$ is the unique random variable that satisfies:
\begin{enumerate}
	\item $\mathbb{E}[X|\mathcal{G}]$ is $\mathcal{G}-measurable$
	\item $\int_A \mathbb{E}[X|\mathcal{G}](w) d\mathbb{P}(w) = \int_A X(w) d\mathbb{P}(w)$, for all $A \in \mathcal{G}$ \\
	(alternative expression: $\forall A \in \mathcal{G}, \mathbb{E}[\mathbb{E}[\mathbbm{1}_{A}X|\mathcal{G}]] = \mathbb{E}[\mathbbm{1}_{A}X]$)
\end{enumerate}
\indent $P.S.$: a very useful thing to remember: $\mathbb{P}(A) = \mathbb{E}\mathbbm{1}_{A}$.

\subsection{Stopping Times}
Consider a measurable space $(\Omega, \mathcal{F})$ equipped with a filtration $\{ \mathcal{F}_t \}$. A random time $T$ is a stopping time w.r.t. that filtration, if the event $\{ T \leq t \}$ belongs to $\mathcal{F}_t$, $\forall t \geq 0$. \\
\indent Let $T$, $S$ be stopping times and $Z$ an integrable $R.V.$. We have:
\begin{enumerate}
	\item $\mathbb{E}[Z|{\mathcal{F}_T}] = \mathbb{E}[Z|{\mathcal{F}_{S \bigwedge T}}]$, P-a.s. on $\{ T \leq S \}$
	\item $\mathbb{E}[\mathbb{E}[Z|{\mathcal{F}_T}]|\mathcal{F}_S] = \mathbb{E}[Z|{\mathcal{F}_{S \bigwedge T}}]$, P-a.s.
\end{enumerate}
\section{Brownian Motion}

\subsection{Construction}

\subsection{Levy Theorem}
Let $M(t), t \geq 0$, be a martingale w.r.t. $\mathcal{F}(t)$. We have $M(0) = 0$, $M(t)$ has continuous sample paths, $\braket{M, M}(t) = t, \forall t \geq 0$.\\ 
$\Longrightarrow M(t)$ is a Brownian motion. \\
\\
\textit{Sketch of Proof:}\\
\\ For any function $f(t, x)$, we have:
\begin{equation}
f(t, M(t)) = f_0 + \int_0^t[f_{t} + \frac{1}{2}f_{xx}]ds + \int_0^t f_x dM(s)
\end{equation}
where we've used $\braket{M, M}(t) =t \rightarrow dM(t)dM(t) = dt$. Taking expectation on both sides, due to the martingale property of $M(t)$, the expectation of the integral w.r.t. $dM(t)$ disappears. Due to the arbitrarity of $f(t, x)$, we can select $f(t, x) = e^{ux - \frac{1}{2}u^2t}$. Thus we obtain:
\begin{subequations}
\begin{align}
f_t + \frac{1}{2}f_{xx} &= 0, \\
\mathbb{E}exp\{uM(t) - \frac{1}{2}u^2 t\} &= e^{0 - 0} = 1, \\
\Longrightarrow \mathbb{E}e^{uM(t)} &= e^{\frac{1}{2}u^2 t}
\end{align}
\end{subequations}
\indent We believe two $R.V.$ who have the same moment fenerating function should have the same distribution. Therefore we prove the normality of $M(t)$.


\subsection{First Passage Time}

\subsection{Maximum of Brownian Motion with Drift}

\section{Ito Integral}
\textbf{Property of $I(t)$: }
\begin{enumerate}
	\item Continuity
	\item $\mathcal{F}(t)-measurable$
	\item Linearity
	\item Martingale
	\item $Isometry:$ $\mathbb{E}I^2(t) = \mathbb{E}\int^t_0{\Delta^2(u) du}$
	\item $QV(t) = [I, I](t) = \int^t_0{\Delta^2(u) du}$ 
\end{enumerate}

\indent There is a useful exercise on Shreve $P_{151} - 4.4.11$.

\section{Risk-Neutral Measure}

\subsection{Change of Measure}
In $(\Omega, \mathcal{F}, \mathbb{P})$, $R.V.$ $Z$ is a.s. nonnegative, $\mathbb{E}Z = 1$.\\
\indent Then for all $A \in \mathcal{F}$, we can define $\widetilde{\mathbb{P}}(A) = \int_A{Z(w)d\mathbb{P}(w)}$.

\subsection{Radon-Nikodym Derivative Process}
We have $(\Omega, \mathcal{F}, \mathbb{P})$ and $filtration$ $\mathcal{F}(t)$ defined on $0 \leq t \leq T$($T$ fixed). $R.V.$ $Z$ is a.s. nonnegative, $\mathbb{E}Z = 1$. Define $\widetilde{\mathbb{P}}$ as in previous subsection.\\
\indent Define the Radon-Nikodym Derivative Process to be $Z(t) = \mathbb{E}[Z|\mathcal{F}(t)]$, $Z(t)$ is a $martingale$ with respect to $\mathcal{F}(t)$.\\
\textbf{Property of $Z(t)$: }
\begin{enumerate}
	\item if $Y$ is a $\mathcal{F}(t)-measurable$ $R.V.$, then $\widetilde{\mathbb{E}}Y = \mathbb{E}[YZ(t)]$
	\item if $0 \leq s \leq t \leq T$, $Y$ is a $\mathcal{F}(t)-measurable$ $R.V.$, then \\$Z(s) \widetilde{\mathbb{E}}[Y|\mathcal{F}(s)] = \mathbb{E}[YZ(t)|\mathcal{F}(s)]$
\end{enumerate}

\subsection{Girsanov Theorem, one dimension}
We have $W(t), 0\leq t \leq T$ on $(\Omega, \mathcal{F}, \mathbb{P})$, and let $\mathcal{F}(t), 0 \leq t \leq T$ be the filtration for $W(t)$ and $\Theta(t)$ be an adapted process to it. Define
\begin{subequations}
\begin{align}
Z(t) &= exp\{-\int_0^t\Theta(u)dW(u) - \frac{1}{2}\int_0^t\Theta^2(u) du\}, \\
\widetilde{W(t)} &= W(t) + \int_0^t\Theta(u)du, \\
assume &\quad \mathbb{E}\int_0^T \Theta^2(u)Z^2(u)du < \infty
\end{align}
\end{subequations}
Set $Z = Z(T)$, it follows:
\begin{equation}
\mathbb{E}Z = 1 
\end{equation}
Define  a new probability measure by:
\begin{equation}
d\widetilde{\mathbb{P}} = Zd\mathbb{P}
\end{equation}
Then under $\widetilde{\mathbb{P}}$ measure, $\widetilde{W(t)}$ is a Brownian motion.

\subsection{Martingale Representation Theorem, one dimension}
We have $W(t), 0\leq t \leq T$ on $(\Omega, \mathcal{F}, \mathbb{P})$, and let $\mathcal{F}(t), 0 \leq t \leq T$ be the filtration for $W(t)$. Let $M(t)$ be a martingale w.r.t. $\mathcal{F}(t)$. 
$\Longrightarrow \exists$ an $\mathcal{F}(t)$ adapted process $\Gamma(u), 0 \leq u \leq T$, such that:
\begin{equation}
M(t) = M(0) + \int_0^t \Gamma(u)dW(u), 0 \leq t \leq T.
\end{equation}
\indent Using the Martingale Representation Theorem \& Girsanov Theorem, it can be proved that: \\
\indent Let $M(t)$ be a martingale under $\widetilde{\mathbb{P}}$. Then there exists an adapted process $\widetilde{\Gamma(u)}$ w.r.t $\mathcal{F}(t)$, such that:
\begin{equation}
\widetilde{M(t)} = \widetilde{M(0)} + \int_0^t \widetilde{\Gamma(u)} \widetilde{dW(u)}, 0 \leq t \leq T.
\end{equation}

\subsection{Application of Risk-Neutral}

\section{Stochastic Differentiation Equations}

\subsection{Markov Property}
Solutions to \textit{S.D.E.} are \textit{Markov processes}.

\subsection{Feynmann-Kac Theorem, one dimension}
Consider the following \textit{S.D.E.}:
\begin{equation}
dX(u) = \beta(u, X(u)) du + \gamma(u, X(u)) dW(u)
\end{equation}
Fix $T > 0$, let $0 \leq t \leq T$. Let $h(y)$ be \textit{Borel-measurable}. Define the function
\begin{equation}
g(t, x) = \mathbb{E}^{t, x}h(X(T))
\end{equation}
$\Longrightarrow$ $g(t, x)$ satisfies \textit{P.D.E.}:
\begin{subequations}
\begin{align}
g_t + \beta g_x + \frac{1}{2}\gamma^2 g_{xx} &= 0 \\
g(T, x) = h(x)
\end{align}
\end{subequations}
\textit{Sketch of Proof: }\\
\indent It follows immediately that the process $g(t, X(t))$ is a martingale.\par
On the other hand, we have
\begin{equation}
dg(t, X(t)) = [g_t + \beta g_x + \frac{1}{2}\gamma^2 g_xx]dt + \gamma g_x dW(t)
\end{equation}
Therefore, the $dt$ term must be $0$.

\subsection{General Version of Feynman-Kac}
For $0 \leq t \leq T$, $x \in \mathbb{R}^d$, $d-dim$ $W(t)$, $\sigma$ is a $d \times d$ diffusion matrix, 
\begin{equation}
X(\theta) = x +\int_t^{\theta}{\mu(s, X(s))ds}+\int_t^{\theta}{\sigma(s, X(s))dW(s)}
\end{equation}
\indent We define operator $\mathcal{A}_t=\sum_i \mu_i \partial_{x_i} + \frac{1}{2}\sum_{i,j}a_{ij}\frac{\partial^2}{\partial{x_i}\partial{x_j}}$, where $a_{ij} = \sum_k \sigma_{ik} \sigma{jk}$.\par
Under some technical conditions which often hold, we have
\begin{equation}
V(t, x) = \mathbb{E}[\int_t^T e^{-\int_t^{\theta}k(u, X(u))du} f(\theta, X(\theta))d\theta + g(X(T)) e^{-\int_t^T k(u, X(u))du}]
\end{equation}
solves the \textit{P.D.E.} with terminal condition:
\begin{subequations}
\begin{align}
\frac{\partial V}{\partial t} + \mathcal{A}_t V + f &= kV \\
V(T, y) = g(y)
\end{align}
\end{subequations}
\indent We can verify $V(t, x)$ solves the equation. \textit{Define}
\begin{subequations}
$\beta(t) = e^{-\int_0^t{k(u, X(u))du}}$, then it follows
\begin{align}
V(t, X(t)) &= \mathbb{E}[\int_t^T \frac{\beta(s)}{\beta(t)}f(s, X(s))ds + \frac{\beta(T)}{\beta(t)}g(X(T))|\mathcal{F}(t)] \\
M(t) &= \beta(t) V(t, X(t)) + \int_0^t \beta(s)f(s, X(s))ds \\
\Longrightarrow M(t) &=  \mathbb{E}[\int_0^T \beta(s)f(s, X(s))ds + \beta(T)g(X(T))|\mathcal{F}(t)]
\end{align}
\end{subequations}
\indent Obviously $M(t)$ is a \textit{Levy martingale}. What is left to show is calculate $dM(t)$ and set $dt$ term to $0$ to obtain the \textit{P.D.E.} $V(t, X(t))$ solves.\par
Notes: 
\begin{enumerate}
\item $V(t, X(t))$ is the asset price at time $t$. In practice, we often change to risk-neutral measure first.
\item $g(y)$ is the final payoff function w.r.t. stock price $y$
\item $k(u, X(u))$ is the interest rate, then $\beta(t)$ is the discount factor
\item Setting $f = 0, k(u, X(u)) = r = const$, and change to risk-neutral measure w.r.t. an underlying stock for a call option, we have the common form of \textit{Discounted Feynman-Kac}:
\begin{equation}
c(t, X(t)) = \mathbb{E}^{\mathbb{Q}}[e^{-r(T-t)}(S(T) - K)^{+}|\mathcal{F}(t)]
\end{equation}
\end{enumerate}

\subsection{Transitional Density}
\textit{Definition:}
\begin{equation}
\mathbb{P}(X(T)\in A | X(t) = x) = \int_A p(t,T,x,y) dy
\end{equation}
\begin{equation}
\mathbb{E}^{t, x}h(X(T)) = \int h(y)p(t, T, x, y)dy
\end{equation}

\begin{equation}
p^{t, x}(T, y) = \int{p(t, T, x, y) p(t, x) dx}
\end{equation}

\subsection{Kolmogorov Backward \& Forward Equation}
$Shreve$ $P_{291}$\par
For process $X(t)$:
\begin{equation}
dX(t) = \beta(t, X(t))dt + \gamma(t, X(t))dW(t)
\end{equation}
\begin{subequations}
Let $\mathcal{A}_t = \beta(t, x)\frac{\partial}{\partial x} + \frac{1}{2} \gamma^2(t, x) \frac{\partial^2}{\partial x^2}$, $\mathcal{A}_t^{\dag} = \beta(t, x)\frac{\partial}{\partial x} - \frac{1}{2} \gamma^2(t, x) \frac{\partial^2}{\partial x^2}$.\\(Note that $t \sim x, T \sim y$) \\
Let the transitional density be $p(t, T, x, y)$.
\begin{align}
(\frac{\partial}{\partial t} + \mathcal{A}_t)p(t, T, x, y) &= 0 \\
(\frac{\partial}{\partial T} - \mathcal{A}_T^{\dag})p(t, T, x, y) &= 0
\end{align}
\end{subequations}
\subsection{Volatility Smile \& Surface}

\section{Excellent Exercise on Courseware}
\subsection{Introduction to SA}
\begin{enumerate}

	\item P.156 the property of infinitismal generator
	\item P.160 Prove the Komogorov  supplem P.55
	\item P.162 Show $V(t, x)$ is the solution to $\partial_t V(t, x) + \mathcal{A}_t V(t, x) + f(t, x) = k(t, x)V(t,x)$ on the previous page.
	\item P.165 top
	\item P.172
	\item P.178
\end{enumerate}

\subsection{Supplementary Notes on Introduction}
\begin{enumerate}
	\item P.26 bottom
	\item P.31
	\item P.50 51
	\item P.53
	\item P.59
\end{enumerate}

\subsection{Application of SA in Financial Engineering}
\begin{enumerate}
	\item P.12 bottom
	\item P.14 prove (3)
	\item P.15 top
	\item P.23 top
	\item P.26 $dX = \sum \Delta_i dSi + r(X - \sum \Delta_i S_i)dt$, prove $d(e^{-rt} X(t)) = \sum \Delta_i d(e^{-rt} S_i(t))$
	\item 
\end{enumerate}




\end{document}
